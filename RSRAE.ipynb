{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Link: https://arxiv.org/abs/2003.04696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchio\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.distributions as dist\n",
    "import math\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = r\"W:\\12cp\\data\\mood.h5\"\n",
    "validation_data = r\"W:\\12cp\\data\\MOOD_toytest_brain.h5\"\n",
    "log_path = r\"W:\\12cp\\log\"\n",
    "save_path = r\"W:\\12cp\\save_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.set_device(0)\n",
    "batch_size = 128\n",
    "num_workers=0\n",
    "trainID=\"RESAES\"\n",
    "learning_rate = 0.00025\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_h5 = True\n",
    "indicesOfImgVols = [1,2] #Supply list of indices if only a subset is desired\n",
    "patch_size=(256,256,1) #Set it to None if not desired\n",
    "patchQ_len = 512\n",
    "patches_per_volume = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_val=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(256,256,256)\n",
    "encode_features=128\n",
    "linear_op = True\n",
    "normalize=True\n",
    "if_rsr=True\n",
    "enforce_proj=True\n",
    "all_alt=False\n",
    "lambda1 = 0.1\n",
    "lambda2 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_region='brain'\n",
    "useCuda=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "from Data import MoodTrainSet, MoodValSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading MoodTrainSet\n",
      "Preloading MoodValSet\n"
     ]
    }
   ],
   "source": [
    "trainset = MoodTrainSet(indices=indicesOfImgVols, region=mood_region, data_path=training_data, lazypatch=True if patch_size else False, preload=preload_h5)\n",
    "valset = MoodValSet(data_path=validation_data, lazypatch=True if patch_size else False, preload=preload_h5)\n",
    "\n",
    "if patch_size:\n",
    "  input_shape = tuple(x for x in patch_size if x!=1)\n",
    "  trainset = torchio.data.Queue(\n",
    "                  subjects_dataset = trainset,\n",
    "                  max_length = patchQ_len,\n",
    "                  samples_per_volume = patches_per_volume,\n",
    "                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n",
    "                  # num_workers = num_workers\n",
    "                  )\n",
    "  valset = torchio.data.Queue(\n",
    "                  subjects_dataset = valset,\n",
    "                  max_length = patchQ_len,\n",
    "                  samples_per_volume = patches_per_volume,\n",
    "                  sampler = torchio.data.UniformSampler(patch_size=patch_size),\n",
    "                  # num_workers = num_workers\n",
    "                  )\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False, num_workers=num_workers)\n",
    "val_loader = None if (valset is None) or (not do_val) else DataLoader(dataset=valset,batch_size=batch_size,shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels, output_channels, kernel_size, stride):\n",
    "  return nn.Sequential(nn.Conv2d(input_channels, output_channels, kernel_size, stride, bias=False),\n",
    "                    nn.BatchNorm2d(output_channels),\n",
    "                    nn.LeakyReLU(0.2, inplace=True))\n",
    "  \n",
    "def linear_enc(input_channels, output_channels):\n",
    "  return nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(in_features=input_channels, out_features=output_channels),\n",
    "      nn.BatchNorm1d(num_features=output_channels),\n",
    "      nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, no_channels, filter_size, latent_size, linear_op=True):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.linear_op = linear_op\n",
    "        self.conv1 = conv_block(no_channels, filter_size, kernel_size=5, stride=2)\n",
    "        self.conv2 = conv_block(filter_size, filter_size*2, kernel_size=5, stride=2)\n",
    "        self.conv3 = conv_block(filter_size*2, filter_size*4, kernel_size=5, stride=2)\n",
    "        self.linear = linear_enc(29*29*filter_size*4, latent_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.linear_op:\n",
    "          x_rsr = self.linear(x)\n",
    "        return x, x_rsr\n",
    "\n",
    "def deconv(input_channels, output_channels, kernel_size, stride):\n",
    "  return nn.Sequential(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, bias=False),\n",
    "                    nn.BatchNorm2d(output_channels),\n",
    "                    nn.LeakyReLU(0.2, inplace=True))\n",
    "  \n",
    "def linear_dec(input_channels, output_channels):\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(in_features=input_channels, out_features=output_channels),\n",
    "      nn.BatchNorm1d(num_features=output_channels),\n",
    "      nn.LeakyReLU(0.2, inplace=True),\n",
    "      Unflatten((128,29,29)))\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.view(len(input), self.shape[0], self.shape[1], self.shape[2])\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, no_channels, filter_size, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.linear = linear_dec(latent_size, 29*29*filter_size*4)\n",
    "        self.deconv1 = deconv(filter_size*4, filter_size*2, 6, 2)\n",
    "        self.deconv2 = deconv(filter_size*2, filter_size, 5, 2)\n",
    "        self.deconv3 = deconv(filter_size, no_channels, 4, 2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.linear(input)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aes(nn.Module):\n",
    "    def __init__(self, no_channels, filter_size, latent_size, normalize=True, linear_op=True):\n",
    "        super(Aes, self).__init__()\n",
    "        self.Encoder = Encoder(no_channels=no_channels, filter_size=filter_size, latent_size=latent_size, linear_op=linear_op)\n",
    "        self.Decoder = Decoder(no_channels=no_channels, filter_size=filter_size, latent_size=latent_size)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, input):\n",
    "        y, y_rsr = self.Encoder(input)\n",
    "        if self.normalize:\n",
    "          z = f.normalize(y_rsr, dim=-1, p=2)\n",
    "        else:\n",
    "          z = y_rsr\n",
    "        x_tilde = self.Decoder(z)\n",
    "        return y, y_rsr, z, x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_error(x, xtilde):\n",
    "  return torch.mean(torch.norm(x-x_tilde, dim=1))\n",
    "\n",
    "def pca_error(y, z):\n",
    "  y = y.reshape(y.shape[0],-1)\n",
    "  A = Variable(torch.randn(y.shape[-1], 128))\n",
    "  z = torch.matmul(z, torch.transpose(A,0,1))\n",
    "  return torch.mean(torch.norm(y-z, dim=1)) # it's 2 or 'fro' by default\n",
    "\n",
    "def proj_error(y, z):\n",
    "  y = y.reshape(y.shape[0],-1)\n",
    "  A = Variable(torch.randn(y.shape[-1], 128))\n",
    "  z = torch.matmul(z, torch.transpose(A,0,1))\n",
    "  return torch.mean(torch.square(torch.matmul(torch.transpose(A,0,1), A) - torch.eye(128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_freq = 10\n",
    "tb_writer = SummaryWriter(log_dir = os.path.join(log_path,trainID))\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "logname = os.path.join(save_path, 'log_'+trainID+'.txt')\n",
    "logging.basicConfig(filename=logname,\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%H:%M:%S',\n",
    "                            level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2load = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aes(no_channels=1, filter_size=32, latent_size=128, normalize=normalize, linear_op=linear_op)\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer2 = Adam(model.parameters(), lr=10*learning_rate)\n",
    "optimizer3 = Adam(model.parameters(), lr=10*learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_rec = nn.MSELoss()\n",
    "runningLoss = 0.0\n",
    "runningLossCounter = 0.0\n",
    "train_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint2load:\n",
    "    chk = torch.load(checkpoint2load)\n",
    "    model.load_state_dict(chk['state_dict'])\n",
    "    optimizer.load_state_dict(chk['optimizer'])\n",
    "    amp.load_state_dict(chk['amp'])\n",
    "    start_epoch = chk['epoch'] + 1\n",
    "    best_loss = chk['loss'] \n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train().to(device)\n",
    "    runningLoss = 0.0\n",
    "    runningLossCounter = 0.0\n",
    "    train_loss = 0.0\n",
    "    print('Epoch '+ str(epoch)+ ': Training')\n",
    "    \n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for i, data in enumerate(train_loader):\n",
    "            try:\n",
    "                img = data['img']['data'].squeeze(-1)\n",
    "                data = Variable(img).to(device)\n",
    "                model.zero_grad()\n",
    "                y, y_rsr, z, x_tilde=model(data)\n",
    "\n",
    "                if if_rsr and not all_alt:\n",
    "                    loss = recon_error(data, x_tilde) + lambda1 * pca_error(y, y_rsr) + lambda2 * proj_error(y, y_rsr)\n",
    "                else:\n",
    "                    loss = recon_error(data, x_tilde)\n",
    "                if not torch.isfinite(loss):\n",
    "                    logging.error('Loss is not finite. Skipping the iteration.')\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if enforce_proj and all_alt:\n",
    "                    loss_proj = proj_error(y, y_rsr)\n",
    "                    loss_proj.backward()\n",
    "                    optimizer2.step()\n",
    "                if all_alt:\n",
    "                    loss_alt = pca_error(y, y_rsr)\n",
    "                    loss_alt.backward()\n",
    "                    optimizer3.step()\n",
    "                loss = round(loss.item(),4)\n",
    "                train_loss += loss\n",
    "                runningLoss += loss\n",
    "                runningLossCounter += 1\n",
    "                logging.info('[%d/%d][%d/%d] Train Loss: %.4f' % ((epoch+1), num_epochs, i, len(train_loader), loss))\n",
    "\n",
    "                if i % log_freq == 0:\n",
    "                    niter = epoch*len(train_loader)+i\n",
    "                    tb_writer.add_scalar('Train/Loss', runningLoss/runningLossCounter, niter)\n",
    "                    runningLoss = 0.0\n",
    "                    runningLossCounter = 0.0\n",
    "            except Exception as e:\n",
    "                print(\"Caught exception: \", e)\n",
    "                logging.error(str(e))\n",
    "                pbar.update(1)\n",
    "            pbar.update(1)\n",
    "    checkpoint = {\n",
    "      'model': model,\n",
    "      'state_dict': model.state_dict(),\n",
    "      'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(save_path, trainID+\".pth.tar\"))\n",
    "    tb_writer.add_scalar('Train/AvgLossEpoch', train_loss/len(train_loader), epoch)\n",
    "    \n",
    "    if val_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print('Epoch '+ str(epoch)+ ': Validation')\n",
    "            with tqdm(total=len(val_loader)) as pbar:\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    try:\n",
    "                        img = data['img']['data'].squeeze(-1)\n",
    "                        images = Variable(img).to(device)\n",
    "                        y, y_rsr, z, x_tilde = model(images)\n",
    "                        loss_rec = recon_error(data, x_tilde)\n",
    "                        loss_pca = pca_error(y, y_rsr)\n",
    "                        loss_proj = proj_error(y, y_rsr)\n",
    "                        loss = loss_rec + loss_pca + loss_proj\n",
    "                        logging.info('[%d/%d][%d/%d] Val Loss: %.4f' % ((epoch+1), num_epochs, i, len(val_loader), loss.mean().item()))\n",
    "                        niter = epoch*len(val_loader)+i\n",
    "                        for j in range(images.size(0)):\n",
    "                            tb_writer.add_scalar('Val/Loss', loss[j].item(), (batch_size*niter)+j)\n",
    "                            tb_writer.add_scalar('Val/GT', y[j], (batch_size*niter)+j)\n",
    "                    except Exception as ex:\n",
    "                        logging.error(ex)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
